{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cd14ec-12c3-4876-b14a-d77d6cd54be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "DATASET_PATH = \"Dataset/\"\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e861d282-3eb5-4d79-be71-427d75759b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da6f8c3-13f9-4c1f-be7d-7cc2bd4028a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_PATH + \"train.csv\")\n",
    "test_df = pd.read_csv(DATASET_PATH + \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58a75403-a5e7-49c6-9914-d75a0fc78eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 1201)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a50b27d2-ba8d-4c99-aa5c-6f9b334a14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "X = train_df.iloc[:, 1:]  # Features (all columns except the first)\n",
    "y = train_df.iloc[:, 0]  # Labels (first column)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = X_train.values, X_val.values, y_train.values, y_val.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f2497-04af-440a-8a04-d3e9a9cad17f",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb5df8bc-4bd2-4061-952e-42b8c591dd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on the validation set: 0.6205128205128205\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_val)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(\"F1 score on the validation set:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35b34c7-e551-47ec-8357-b5e455b62d17",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7e12d99-8183-4ff1-b806-319e395b4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data into the required format (assuming images are 20x20x3)\n",
    "X_train = X_train.reshape(-1, 20, 20, 3)\n",
    "X_val = X_val.reshape(-1, 20, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "463cc1e3-7784-460b-9fd7-f4e3da43421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 1s 3ms/step - loss: 0.5912 - f1_m: 0.0031 - val_loss: 0.5750 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.5823 - f1_m: 0.0000e+00 - val_loss: 0.5649 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.5650 - f1_m: 0.0000e+00 - val_loss: 0.5230 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.4957 - f1_m: 0.0000e+00 - val_loss: 0.4610 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.4525 - f1_m: 0.1156 - val_loss: 0.4504 - val_f1_m: 0.3919\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.4395 - f1_m: 0.3693 - val_loss: 0.4366 - val_f1_m: 0.4198\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.4201 - f1_m: 0.4550 - val_loss: 0.4207 - val_f1_m: 0.4076\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3936 - f1_m: 0.5189 - val_loss: 0.4045 - val_f1_m: 0.5304\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3781 - f1_m: 0.5658 - val_loss: 0.3900 - val_f1_m: 0.5218\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3525 - f1_m: 0.5918 - val_loss: 0.3913 - val_f1_m: 0.5608\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3344 - f1_m: 0.6385 - val_loss: 0.3724 - val_f1_m: 0.5464\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3142 - f1_m: 0.6669 - val_loss: 0.3795 - val_f1_m: 0.5462\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.3001 - f1_m: 0.6895 - val_loss: 0.3689 - val_f1_m: 0.5733\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.2828 - f1_m: 0.7118 - val_loss: 0.3764 - val_f1_m: 0.5748\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.2668 - f1_m: 0.7330 - val_loss: 0.3730 - val_f1_m: 0.5871\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.2464 - f1_m: 0.7619 - val_loss: 0.3836 - val_f1_m: 0.5650\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.2354 - f1_m: 0.7925 - val_loss: 0.3815 - val_f1_m: 0.5876\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.2133 - f1_m: 0.8125 - val_loss: 0.4125 - val_f1_m: 0.6034\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1952 - f1_m: 0.8421 - val_loss: 0.4331 - val_f1_m: 0.6089\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1810 - f1_m: 0.8718 - val_loss: 0.4131 - val_f1_m: 0.5875\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1614 - f1_m: 0.8915 - val_loss: 0.4215 - val_f1_m: 0.6019\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1409 - f1_m: 0.9080 - val_loss: 0.4276 - val_f1_m: 0.5842\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1256 - f1_m: 0.9202 - val_loss: 0.4464 - val_f1_m: 0.5802\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.1098 - f1_m: 0.9416 - val_loss: 0.4691 - val_f1_m: 0.6052\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0952 - f1_m: 0.9588 - val_loss: 0.4981 - val_f1_m: 0.6007\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0806 - f1_m: 0.9673 - val_loss: 0.5058 - val_f1_m: 0.6099\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0669 - f1_m: 0.9790 - val_loss: 0.5253 - val_f1_m: 0.6142\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0545 - f1_m: 0.9918 - val_loss: 0.5581 - val_f1_m: 0.5988\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0443 - f1_m: 0.9977 - val_loss: 0.5625 - val_f1_m: 0.6143\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0371 - f1_m: 0.9989 - val_loss: 0.5849 - val_f1_m: 0.6138\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0301 - f1_m: 1.0000 - val_loss: 0.6031 - val_f1_m: 0.6197\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0253 - f1_m: 1.0000 - val_loss: 0.6283 - val_f1_m: 0.6137\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0197 - f1_m: 0.9924 - val_loss: 0.6490 - val_f1_m: 0.6175\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0168 - f1_m: 1.0000 - val_loss: 0.6639 - val_f1_m: 0.6154\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0136 - f1_m: 1.0000 - val_loss: 0.6900 - val_f1_m: 0.6196\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0114 - f1_m: 1.0000 - val_loss: 0.7079 - val_f1_m: 0.6190\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0095 - f1_m: 1.0000 - val_loss: 0.7261 - val_f1_m: 0.6227\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0082 - f1_m: 1.0000 - val_loss: 0.7447 - val_f1_m: 0.6200\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0069 - f1_m: 1.0000 - val_loss: 0.7680 - val_f1_m: 0.6068\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0059 - f1_m: 1.0000 - val_loss: 0.7784 - val_f1_m: 0.6217\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0051 - f1_m: 1.0000 - val_loss: 0.7972 - val_f1_m: 0.6187\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0044 - f1_m: 1.0000 - val_loss: 0.8113 - val_f1_m: 0.6253\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0039 - f1_m: 1.0000 - val_loss: 0.8278 - val_f1_m: 0.6228\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0033 - f1_m: 1.0000 - val_loss: 0.8417 - val_f1_m: 0.6234\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0030 - f1_m: 1.0000 - val_loss: 0.8574 - val_f1_m: 0.6252\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0026 - f1_m: 1.0000 - val_loss: 0.8706 - val_f1_m: 0.6256\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0023 - f1_m: 1.0000 - val_loss: 0.8844 - val_f1_m: 0.6285\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0020 - f1_m: 1.0000 - val_loss: 0.8982 - val_f1_m: 0.6305\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0018 - f1_m: 1.0000 - val_loss: 0.9110 - val_f1_m: 0.6213\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 3ms/step - loss: 0.0016 - f1_m: 1.0000 - val_loss: 0.9320 - val_f1_m: 0.6023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x166465a10>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(20, 20, 3)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_m])\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52e3da26-d2e3-4387-bee3-ac5c18514a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 1ms/step\n",
      "F1 score on the validation set: 0.6220472440944882\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(\"F1 score on the validation set:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0ab38-80c7-4641-86f9-76156973d260",
   "metadata": {},
   "source": [
    "## Complex CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9100d72e-a6bd-4d13-8419-2acf49225e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.5910 - f1_m: 0.0000e+00 - val_loss: 0.5693 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.5848 - f1_m: 0.0000e+00 - val_loss: 0.5756 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.5860 - f1_m: 0.0000e+00 - val_loss: 0.5720 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.5852 - f1_m: 0.0000e+00 - val_loss: 0.5690 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.5834 - f1_m: 0.0000e+00 - val_loss: 0.5701 - val_f1_m: 0.0000e+00\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.5833 - f1_m: 0.0000e+00 - val_loss: 0.5700 - val_f1_m: 0.0000e+00\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.5244 - f1_m: 0.0264 - val_loss: 0.4677 - val_f1_m: 0.0000e+00\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.4608 - f1_m: 0.1531 - val_loss: 0.4435 - val_f1_m: 0.3416\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.4455 - f1_m: 0.4166 - val_loss: 0.4095 - val_f1_m: 0.4604\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.4064 - f1_m: 0.5002 - val_loss: 0.4206 - val_f1_m: 0.3871\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3880 - f1_m: 0.5206 - val_loss: 0.3930 - val_f1_m: 0.5329\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3726 - f1_m: 0.5369 - val_loss: 0.3755 - val_f1_m: 0.5362\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3677 - f1_m: 0.5300 - val_loss: 0.3723 - val_f1_m: 0.5722\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3635 - f1_m: 0.5357 - val_loss: 0.3746 - val_f1_m: 0.4690\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3509 - f1_m: 0.5760 - val_loss: 0.3735 - val_f1_m: 0.5376\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3467 - f1_m: 0.5859 - val_loss: 0.3779 - val_f1_m: 0.5319\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3426 - f1_m: 0.5708 - val_loss: 0.3717 - val_f1_m: 0.5194\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3402 - f1_m: 0.5863 - val_loss: 0.3956 - val_f1_m: 0.5479\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3396 - f1_m: 0.5822 - val_loss: 0.3803 - val_f1_m: 0.5392\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3388 - f1_m: 0.5902 - val_loss: 0.3890 - val_f1_m: 0.5220\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3424 - f1_m: 0.5888 - val_loss: 0.3846 - val_f1_m: 0.5375\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3305 - f1_m: 0.6037 - val_loss: 0.4184 - val_f1_m: 0.5501\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3340 - f1_m: 0.6042 - val_loss: 0.3996 - val_f1_m: 0.5316\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.3411 - f1_m: 0.5829 - val_loss: 0.4078 - val_f1_m: 0.5265\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3370 - f1_m: 0.5989 - val_loss: 0.3915 - val_f1_m: 0.5084\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3317 - f1_m: 0.6015 - val_loss: 0.3999 - val_f1_m: 0.5165\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3296 - f1_m: 0.6056 - val_loss: 0.4014 - val_f1_m: 0.5077\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3303 - f1_m: 0.6142 - val_loss: 0.4695 - val_f1_m: 0.5481\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.3241 - f1_m: 0.6278 - val_loss: 0.4333 - val_f1_m: 0.5052\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3123 - f1_m: 0.6309 - val_loss: 0.4477 - val_f1_m: 0.5271\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3105 - f1_m: 0.6439 - val_loss: 0.4291 - val_f1_m: 0.4860\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.3097 - f1_m: 0.6411 - val_loss: 0.4734 - val_f1_m: 0.5283\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 5s 36ms/step - loss: 0.3085 - f1_m: 0.6316 - val_loss: 0.4565 - val_f1_m: 0.5425\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3019 - f1_m: 0.6519 - val_loss: 0.4575 - val_f1_m: 0.5056\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3038 - f1_m: 0.6500 - val_loss: 0.4694 - val_f1_m: 0.5041\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.3087 - f1_m: 0.6307 - val_loss: 0.4564 - val_f1_m: 0.5273\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.3011 - f1_m: 0.6449 - val_loss: 0.4583 - val_f1_m: 0.5449\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.2984 - f1_m: 0.6538 - val_loss: 0.5145 - val_f1_m: 0.5188\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2935 - f1_m: 0.6531 - val_loss: 0.4903 - val_f1_m: 0.5130\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.2955 - f1_m: 0.6423 - val_loss: 0.5384 - val_f1_m: 0.5304\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 5s 39ms/step - loss: 0.2884 - f1_m: 0.6651 - val_loss: 0.5710 - val_f1_m: 0.5331\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2913 - f1_m: 0.6533 - val_loss: 0.4940 - val_f1_m: 0.5051\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.2964 - f1_m: 0.6424 - val_loss: 0.5196 - val_f1_m: 0.5012\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2901 - f1_m: 0.6505 - val_loss: 0.5478 - val_f1_m: 0.5496\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2886 - f1_m: 0.6747 - val_loss: 0.6226 - val_f1_m: 0.5518\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2830 - f1_m: 0.6750 - val_loss: 0.5390 - val_f1_m: 0.5137\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2809 - f1_m: 0.6573 - val_loss: 0.5993 - val_f1_m: 0.5266\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.2817 - f1_m: 0.6773 - val_loss: 0.6530 - val_f1_m: 0.5297\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 5s 38ms/step - loss: 0.2813 - f1_m: 0.6642 - val_loss: 0.5768 - val_f1_m: 0.5092\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 5s 37ms/step - loss: 0.2777 - f1_m: 0.6844 - val_loss: 0.5631 - val_f1_m: 0.5295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1664206d0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the CNN model architecture\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(20, 20, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_m])\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "514742c3-8d95-422b-9acd-297b60473b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 11ms/step\n",
      "F1 score on the validation set: 0.5542168674698795\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(\"F1 score on the validation set:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e2a41-3d41-48ab-9da9-b31d0743e588",
   "metadata": {},
   "source": [
    "## MesoNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d870d568-718b-4ca1-bd3e-a1f9ef17ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features and labels\n",
    "X = train_df.iloc[:, 1:]  # Features (all columns except the first)\n",
    "y = train_df.iloc[:, 0]  # Labels (first column)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = X_train.values, X_val.values, y_train.values, y_val.values\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_val = X_val.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data into the required format (assuming images are 20x20x3)\n",
    "X_train = X_train.reshape(-1, 20, 20, 3)\n",
    "X_val = X_val.reshape(-1, 20, 20, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b22d61e1-dfb7-44bf-ab8e-73b72b598dda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.7624 - f1_m: 0.3718 - val_loss: 0.6512 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.4793 - f1_m: 0.4776 - val_loss: 0.5889 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.4093 - f1_m: 0.5235 - val_loss: 0.5691 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.3683 - f1_m: 0.5948 - val_loss: 0.6183 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.3343 - f1_m: 0.6270 - val_loss: 0.3756 - val_f1_m: 0.4539\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.2931 - f1_m: 0.6994 - val_loss: 0.8787 - val_f1_m: 0.0850\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.2617 - f1_m: 0.7527 - val_loss: 0.4273 - val_f1_m: 0.5768\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.2229 - f1_m: 0.8159 - val_loss: 0.6263 - val_f1_m: 0.5337\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1830 - f1_m: 0.8442 - val_loss: 0.4055 - val_f1_m: 0.5519\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1251 - f1_m: 0.9153 - val_loss: 0.6405 - val_f1_m: 0.5541\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.1205 - f1_m: 0.9016 - val_loss: 0.6867 - val_f1_m: 0.6034\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0781 - f1_m: 0.9466 - val_loss: 0.6428 - val_f1_m: 0.5903\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0461 - f1_m: 0.9695 - val_loss: 0.9130 - val_f1_m: 0.5749\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0417 - f1_m: 0.9763 - val_loss: 1.7526 - val_f1_m: 0.5512\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0594 - f1_m: 0.9617 - val_loss: 0.9649 - val_f1_m: 0.5580\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0543 - f1_m: 0.9615 - val_loss: 1.0029 - val_f1_m: 0.5844\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0339 - f1_m: 0.9822 - val_loss: 1.2230 - val_f1_m: 0.5653\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0287 - f1_m: 0.9817 - val_loss: 1.1321 - val_f1_m: 0.5761\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0295 - f1_m: 0.9773 - val_loss: 1.0439 - val_f1_m: 0.5995\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0406 - f1_m: 0.9744 - val_loss: 1.0036 - val_f1_m: 0.5859\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0451 - f1_m: 0.9724 - val_loss: 0.9721 - val_f1_m: 0.5767\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0309 - f1_m: 0.9822 - val_loss: 1.1583 - val_f1_m: 0.5808\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0192 - f1_m: 0.9878 - val_loss: 1.3852 - val_f1_m: 0.5883\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0239 - f1_m: 0.9866 - val_loss: 1.2071 - val_f1_m: 0.5855\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0254 - f1_m: 0.9844 - val_loss: 1.4472 - val_f1_m: 0.5702\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0297 - f1_m: 0.9763 - val_loss: 1.2879 - val_f1_m: 0.5894\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0245 - f1_m: 0.9850 - val_loss: 1.1772 - val_f1_m: 0.6029\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0145 - f1_m: 0.9843 - val_loss: 1.4127 - val_f1_m: 0.5878\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0126 - f1_m: 0.9918 - val_loss: 1.5103 - val_f1_m: 0.5929\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0541 - f1_m: 0.9618 - val_loss: 1.1325 - val_f1_m: 0.5662\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0159 - f1_m: 0.9915 - val_loss: 1.2706 - val_f1_m: 0.6014\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0106 - f1_m: 0.9943 - val_loss: 1.6496 - val_f1_m: 0.5842\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0143 - f1_m: 0.9910 - val_loss: 1.6130 - val_f1_m: 0.5714\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0053 - f1_m: 0.9961 - val_loss: 1.9165 - val_f1_m: 0.5597\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0120 - f1_m: 0.9898 - val_loss: 1.9526 - val_f1_m: 0.5526\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0537 - f1_m: 0.9652 - val_loss: 1.0894 - val_f1_m: 0.5661\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0150 - f1_m: 0.9927 - val_loss: 1.5071 - val_f1_m: 0.5581\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0019 - f1_m: 1.0000 - val_loss: 1.7642 - val_f1_m: 0.5632\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.7738e-04 - f1_m: 1.0000 - val_loss: 1.9314 - val_f1_m: 0.5683\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.6524e-04 - f1_m: 1.0000 - val_loss: 2.0184 - val_f1_m: 0.5649\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 5.8758e-04 - f1_m: 0.9924 - val_loss: 2.1260 - val_f1_m: 0.5671\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.3975e-04 - f1_m: 1.0000 - val_loss: 2.2027 - val_f1_m: 0.5647\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.5353e-04 - f1_m: 1.0000 - val_loss: 2.2948 - val_f1_m: 0.5644\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.2313e-04 - f1_m: 1.0000 - val_loss: 2.3852 - val_f1_m: 0.5733\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.0858e-04 - f1_m: 1.0000 - val_loss: 2.4560 - val_f1_m: 0.5748\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.1219e-04 - f1_m: 1.0000 - val_loss: 2.5175 - val_f1_m: 0.5699\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8474e-04 - f1_m: 1.0000 - val_loss: 2.5723 - val_f1_m: 0.5667\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4243e-04 - f1_m: 1.0000 - val_loss: 2.6175 - val_f1_m: 0.5709\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.5082e-05 - f1_m: 1.0000 - val_loss: 2.6748 - val_f1_m: 0.5711\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.5457e-05 - f1_m: 1.0000 - val_loss: 2.7239 - val_f1_m: 0.5698\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.4979e-05 - f1_m: 1.0000 - val_loss: 2.7730 - val_f1_m: 0.5673\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.7991e-05 - f1_m: 1.0000 - val_loss: 2.8402 - val_f1_m: 0.5683\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.0676e-05 - f1_m: 1.0000 - val_loss: 2.9112 - val_f1_m: 0.5683\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.9206e-05 - f1_m: 1.0000 - val_loss: 2.9586 - val_f1_m: 0.5683\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.0913e-04 - f1_m: 1.0000 - val_loss: 3.0292 - val_f1_m: 0.5769\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 7.0532e-05 - f1_m: 1.0000 - val_loss: 3.1052 - val_f1_m: 0.5773\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.1229e-04 - f1_m: 0.9918 - val_loss: 3.1460 - val_f1_m: 0.5798\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.2225e-05 - f1_m: 1.0000 - val_loss: 3.2034 - val_f1_m: 0.5772\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 3.8398e-05 - f1_m: 1.0000 - val_loss: 3.2717 - val_f1_m: 0.5748\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 4.6582e-05 - f1_m: 1.0000 - val_loss: 3.3418 - val_f1_m: 0.5786\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 4.0250e-05 - f1_m: 0.9924 - val_loss: 3.3484 - val_f1_m: 0.5760\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 5.7848e-05 - f1_m: 1.0000 - val_loss: 3.3321 - val_f1_m: 0.5682\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.7705e-05 - f1_m: 1.0000 - val_loss: 3.3403 - val_f1_m: 0.5646\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.9229e-05 - f1_m: 0.9924 - val_loss: 3.4035 - val_f1_m: 0.5714\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 6.7919e-05 - f1_m: 1.0000 - val_loss: 3.5579 - val_f1_m: 0.5761\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.6705e-05 - f1_m: 1.0000 - val_loss: 3.5541 - val_f1_m: 0.5709\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.5261e-05 - f1_m: 0.9924 - val_loss: 3.5957 - val_f1_m: 0.5676\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 5.3979e-06 - f1_m: 1.0000 - val_loss: 3.6200 - val_f1_m: 0.5630\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 4.2811e-05 - f1_m: 0.9924 - val_loss: 3.6826 - val_f1_m: 0.5735\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.1901e-05 - f1_m: 1.0000 - val_loss: 3.7403 - val_f1_m: 0.5719\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.8652e-05 - f1_m: 1.0000 - val_loss: 3.8001 - val_f1_m: 0.5678\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 4.5802e-06 - f1_m: 1.0000 - val_loss: 3.8345 - val_f1_m: 0.5731\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 5.0648e-05 - f1_m: 1.0000 - val_loss: 3.8234 - val_f1_m: 0.5713\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 4.2949e-06 - f1_m: 1.0000 - val_loss: 3.9071 - val_f1_m: 0.5724\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.4890e-06 - f1_m: 1.0000 - val_loss: 4.0053 - val_f1_m: 0.5662\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 1s 10ms/step - loss: 1.1598e-05 - f1_m: 1.0000 - val_loss: 4.0550 - val_f1_m: 0.5628\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.6248e-04 - f1_m: 1.0000 - val_loss: 4.1311 - val_f1_m: 0.5689\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.1574 - f1_m: 0.8865 - val_loss: 1.4415 - val_f1_m: 0.5107\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1432 - f1_m: 0.8784 - val_loss: 1.3604 - val_f1_m: 0.5610\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0368 - f1_m: 0.9802 - val_loss: 0.9867 - val_f1_m: 0.5761\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0186 - f1_m: 0.9896 - val_loss: 1.3355 - val_f1_m: 0.5695\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0130 - f1_m: 0.9929 - val_loss: 1.5550 - val_f1_m: 0.5893\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0422 - f1_m: 0.9708 - val_loss: 1.1990 - val_f1_m: 0.5493\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0218 - f1_m: 0.9854 - val_loss: 1.3344 - val_f1_m: 0.5585\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0119 - f1_m: 0.9922 - val_loss: 1.8301 - val_f1_m: 0.5697\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0127 - f1_m: 0.9848 - val_loss: 1.6408 - val_f1_m: 0.5842\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0090 - f1_m: 0.9952 - val_loss: 1.8424 - val_f1_m: 0.5873\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0205 - f1_m: 0.9797 - val_loss: 1.5953 - val_f1_m: 0.5748\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0144 - f1_m: 0.9939 - val_loss: 1.5477 - val_f1_m: 0.5722\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0075 - f1_m: 0.9952 - val_loss: 1.7737 - val_f1_m: 0.5859\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0023 - f1_m: 0.9995 - val_loss: 1.9866 - val_f1_m: 0.5952\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0028 - f1_m: 0.9991 - val_loss: 2.1154 - val_f1_m: 0.5809\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0024 - f1_m: 0.9987 - val_loss: 2.1546 - val_f1_m: 0.5866\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0188 - f1_m: 0.9927 - val_loss: 2.3605 - val_f1_m: 0.5617\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0425 - f1_m: 0.9716 - val_loss: 1.1621 - val_f1_m: 0.5828\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0149 - f1_m: 0.9903 - val_loss: 1.4875 - val_f1_m: 0.5873\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0038 - f1_m: 0.9895 - val_loss: 1.7185 - val_f1_m: 0.5791\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.4379e-04 - f1_m: 1.0000 - val_loss: 1.8371 - val_f1_m: 0.5840\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.5494e-04 - f1_m: 1.0000 - val_loss: 2.0020 - val_f1_m: 0.5910\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.6129e-04 - f1_m: 0.9924 - val_loss: 2.1166 - val_f1_m: 0.5882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2f35887d0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    x = Input(shape=(20, 20, 3))\n",
    "    x1 = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "    x2 = Conv2D(32, (5, 5), padding='same', activation='relu')(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "    x3 = Conv2D(32, (5, 5), padding='same', activation='relu')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "    x4 = Conv2D(64, (5, 5), padding='same', activation='relu')(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "    y = Flatten()(x4)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(32)(y)\n",
    "    y = LeakyReLU(alpha=0.1)(y)\n",
    "    y = Dense(16)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1, activation='sigmoid')(y)\n",
    "    return tf.keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_m])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "95b90ca4-682a-4e33-a6fa-91037cd8b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 0s 3ms/step\n",
      "F1 score on the validation set: 0.609375\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(\"F1 score on the validation set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef0ace8b-b52b-4b79-8e2e-b58d43c019df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.8024 - f1_m: 0.3277 - val_loss: 0.6535 - val_f1_m: 0.0000e+00\n",
      "Epoch 2/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.4893 - f1_m: 0.4348 - val_loss: 0.5703 - val_f1_m: 0.0000e+00\n",
      "Epoch 3/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.4281 - f1_m: 0.5272 - val_loss: 0.6103 - val_f1_m: 0.0000e+00\n",
      "Epoch 4/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.3665 - f1_m: 0.6132 - val_loss: 0.6062 - val_f1_m: 0.0000e+00\n",
      "Epoch 5/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.3157 - f1_m: 0.7073 - val_loss: 0.6666 - val_f1_m: 0.1564\n",
      "Epoch 6/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.2715 - f1_m: 0.7478 - val_loss: 0.3819 - val_f1_m: 0.5843\n",
      "Epoch 7/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.2289 - f1_m: 0.8048 - val_loss: 0.5322 - val_f1_m: 0.6052\n",
      "Epoch 8/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.1711 - f1_m: 0.8580 - val_loss: 0.6543 - val_f1_m: 0.5504\n",
      "Epoch 9/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.1712 - f1_m: 0.8545 - val_loss: 0.8547 - val_f1_m: 0.5426\n",
      "Epoch 10/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0953 - f1_m: 0.9330 - val_loss: 0.7022 - val_f1_m: 0.5941\n",
      "Epoch 11/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0767 - f1_m: 0.9423 - val_loss: 0.7809 - val_f1_m: 0.5862\n",
      "Epoch 12/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0674 - f1_m: 0.9520 - val_loss: 1.1001 - val_f1_m: 0.4914\n",
      "Epoch 13/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0586 - f1_m: 0.9628 - val_loss: 1.2235 - val_f1_m: 0.5807\n",
      "Epoch 14/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0391 - f1_m: 0.9677 - val_loss: 1.7716 - val_f1_m: 0.5683\n",
      "Epoch 15/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0818 - f1_m: 0.9371 - val_loss: 1.0434 - val_f1_m: 0.5642\n",
      "Epoch 16/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0476 - f1_m: 0.9713 - val_loss: 1.0661 - val_f1_m: 0.5865\n",
      "Epoch 17/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0257 - f1_m: 0.9868 - val_loss: 1.2498 - val_f1_m: 0.5984\n",
      "Epoch 18/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0138 - f1_m: 0.9929 - val_loss: 1.6585 - val_f1_m: 0.6005\n",
      "Epoch 19/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0108 - f1_m: 0.9853 - val_loss: 1.4818 - val_f1_m: 0.5808\n",
      "Epoch 20/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0533 - f1_m: 0.9651 - val_loss: 1.1748 - val_f1_m: 0.5530\n",
      "Epoch 21/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0529 - f1_m: 0.9633 - val_loss: 1.1028 - val_f1_m: 0.6070\n",
      "Epoch 22/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0228 - f1_m: 0.9906 - val_loss: 1.4410 - val_f1_m: 0.5926\n",
      "Epoch 23/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0119 - f1_m: 0.9825 - val_loss: 1.4776 - val_f1_m: 0.5819\n",
      "Epoch 24/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0305 - f1_m: 0.9802 - val_loss: 1.3746 - val_f1_m: 0.5685\n",
      "Epoch 25/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0435 - f1_m: 0.9745 - val_loss: 1.1313 - val_f1_m: 0.5979\n",
      "Epoch 26/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0120 - f1_m: 0.9948 - val_loss: 1.3810 - val_f1_m: 0.5930\n",
      "Epoch 27/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0120 - f1_m: 0.9907 - val_loss: 1.3896 - val_f1_m: 0.6071\n",
      "Epoch 28/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0285 - f1_m: 0.9817 - val_loss: 1.8504 - val_f1_m: 0.5898\n",
      "Epoch 29/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0173 - f1_m: 0.9881 - val_loss: 1.2069 - val_f1_m: 0.5952\n",
      "Epoch 30/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0098 - f1_m: 0.9941 - val_loss: 1.7728 - val_f1_m: 0.6105\n",
      "Epoch 31/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0121 - f1_m: 0.9941 - val_loss: 1.4688 - val_f1_m: 0.5829\n",
      "Epoch 32/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0217 - f1_m: 0.9844 - val_loss: 1.4887 - val_f1_m: 0.5850\n",
      "Epoch 33/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0308 - f1_m: 0.9778 - val_loss: 1.3481 - val_f1_m: 0.5898\n",
      "Epoch 34/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0188 - f1_m: 0.9811 - val_loss: 1.4101 - val_f1_m: 0.6126\n",
      "Epoch 35/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0119 - f1_m: 0.9931 - val_loss: 1.9392 - val_f1_m: 0.5778\n",
      "Epoch 36/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0333 - f1_m: 0.9799 - val_loss: 1.3976 - val_f1_m: 0.5710\n",
      "Epoch 37/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0183 - f1_m: 0.9900 - val_loss: 1.4544 - val_f1_m: 0.6009\n",
      "Epoch 38/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0113 - f1_m: 0.9944 - val_loss: 1.7756 - val_f1_m: 0.6122\n",
      "Epoch 39/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0070 - f1_m: 0.9980 - val_loss: 2.3949 - val_f1_m: 0.5759\n",
      "Epoch 40/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0199 - f1_m: 0.9887 - val_loss: 1.8402 - val_f1_m: 0.6002\n",
      "Epoch 41/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0247 - f1_m: 0.9875 - val_loss: 1.3387 - val_f1_m: 0.6174\n",
      "Epoch 42/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0322 - f1_m: 0.9820 - val_loss: 1.2830 - val_f1_m: 0.5849\n",
      "Epoch 43/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0116 - f1_m: 0.9934 - val_loss: 1.6821 - val_f1_m: 0.5932\n",
      "Epoch 44/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0038 - f1_m: 0.9975 - val_loss: 1.8465 - val_f1_m: 0.6023\n",
      "Epoch 45/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0022 - f1_m: 0.9992 - val_loss: 2.1168 - val_f1_m: 0.5945\n",
      "Epoch 46/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0029 - f1_m: 0.9980 - val_loss: 2.0828 - val_f1_m: 0.5898\n",
      "Epoch 47/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0158 - f1_m: 0.9901 - val_loss: 1.6268 - val_f1_m: 0.5885\n",
      "Epoch 48/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0217 - f1_m: 0.9837 - val_loss: 1.6209 - val_f1_m: 0.6161\n",
      "Epoch 49/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0271 - f1_m: 0.9828 - val_loss: 1.4392 - val_f1_m: 0.5762\n",
      "Epoch 50/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0062 - f1_m: 0.9972 - val_loss: 1.9919 - val_f1_m: 0.5738\n",
      "Epoch 51/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0012 - f1_m: 1.0000 - val_loss: 2.0839 - val_f1_m: 0.5954\n",
      "Epoch 52/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.5742e-04 - f1_m: 1.0000 - val_loss: 2.1791 - val_f1_m: 0.5863\n",
      "Epoch 53/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.8739e-04 - f1_m: 1.0000 - val_loss: 2.3270 - val_f1_m: 0.5752\n",
      "Epoch 54/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8939e-04 - f1_m: 0.9924 - val_loss: 2.4821 - val_f1_m: 0.5924\n",
      "Epoch 55/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.2038e-04 - f1_m: 0.9995 - val_loss: 2.5505 - val_f1_m: 0.5941\n",
      "Epoch 56/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.3267e-04 - f1_m: 1.0000 - val_loss: 2.6148 - val_f1_m: 0.5936\n",
      "Epoch 57/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.5440e-04 - f1_m: 1.0000 - val_loss: 2.7287 - val_f1_m: 0.5902\n",
      "Epoch 58/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1388e-04 - f1_m: 1.0000 - val_loss: 2.7937 - val_f1_m: 0.5890\n",
      "Epoch 59/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0285e-04 - f1_m: 1.0000 - val_loss: 2.8938 - val_f1_m: 0.5859\n",
      "Epoch 60/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 6.5994e-05 - f1_m: 1.0000 - val_loss: 2.9368 - val_f1_m: 0.5853\n",
      "Epoch 61/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.1105e-05 - f1_m: 1.0000 - val_loss: 3.0018 - val_f1_m: 0.5853\n",
      "Epoch 62/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 7.9481e-05 - f1_m: 1.0000 - val_loss: 3.0512 - val_f1_m: 0.5867\n",
      "Epoch 63/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1095e-04 - f1_m: 1.0000 - val_loss: 3.3197 - val_f1_m: 0.5663\n",
      "Epoch 64/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.5914e-04 - f1_m: 1.0000 - val_loss: 3.3859 - val_f1_m: 0.5891\n",
      "Epoch 65/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.3513e-05 - f1_m: 1.0000 - val_loss: 3.3984 - val_f1_m: 0.5748\n",
      "Epoch 66/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.0250e-05 - f1_m: 1.0000 - val_loss: 3.4365 - val_f1_m: 0.5748\n",
      "Epoch 67/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.1920e-04 - f1_m: 0.9989 - val_loss: 3.2868 - val_f1_m: 0.5672\n",
      "Epoch 68/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1517 - f1_m: 0.8917 - val_loss: 0.7296 - val_f1_m: 0.4953\n",
      "Epoch 69/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0469 - f1_m: 0.9673 - val_loss: 0.9766 - val_f1_m: 0.5799\n",
      "Epoch 70/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0240 - f1_m: 0.9877 - val_loss: 1.3181 - val_f1_m: 0.5919\n",
      "Epoch 71/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0295 - f1_m: 0.9823 - val_loss: 1.2946 - val_f1_m: 0.5877\n",
      "Epoch 72/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0101 - f1_m: 0.9925 - val_loss: 1.3954 - val_f1_m: 0.5799\n",
      "Epoch 73/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0222 - f1_m: 0.9844 - val_loss: 1.2581 - val_f1_m: 0.6170\n",
      "Epoch 74/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0060 - f1_m: 0.9948 - val_loss: 1.6920 - val_f1_m: 0.5933\n",
      "Epoch 75/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0096 - f1_m: 0.9939 - val_loss: 1.9038 - val_f1_m: 0.5903\n",
      "Epoch 76/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0140 - f1_m: 0.9914 - val_loss: 1.5869 - val_f1_m: 0.5965\n",
      "Epoch 77/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0173 - f1_m: 0.9888 - val_loss: 1.5373 - val_f1_m: 0.6018\n",
      "Epoch 78/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0034 - f1_m: 0.9977 - val_loss: 1.9671 - val_f1_m: 0.5989\n",
      "Epoch 79/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 6.1643e-04 - f1_m: 1.0000 - val_loss: 2.1205 - val_f1_m: 0.6022\n",
      "Epoch 80/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.0923e-04 - f1_m: 1.0000 - val_loss: 2.2277 - val_f1_m: 0.6058\n",
      "Epoch 81/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.1696e-04 - f1_m: 1.0000 - val_loss: 2.3460 - val_f1_m: 0.6011\n",
      "Epoch 82/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.9438e-04 - f1_m: 1.0000 - val_loss: 2.4512 - val_f1_m: 0.6019\n",
      "Epoch 83/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.0449e-04 - f1_m: 1.0000 - val_loss: 2.5537 - val_f1_m: 0.6031\n",
      "Epoch 84/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1136e-04 - f1_m: 1.0000 - val_loss: 2.6505 - val_f1_m: 0.5999\n",
      "Epoch 85/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.4094e-05 - f1_m: 1.0000 - val_loss: 2.7046 - val_f1_m: 0.6029\n",
      "Epoch 86/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.6713e-05 - f1_m: 1.0000 - val_loss: 2.7436 - val_f1_m: 0.5992\n",
      "Epoch 87/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.3801e-05 - f1_m: 1.0000 - val_loss: 2.7977 - val_f1_m: 0.6107\n",
      "Epoch 88/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.5570e-05 - f1_m: 1.0000 - val_loss: 2.8522 - val_f1_m: 0.6013\n",
      "Epoch 89/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.3654e-05 - f1_m: 0.9924 - val_loss: 2.9210 - val_f1_m: 0.6062\n",
      "Epoch 90/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7377e-05 - f1_m: 1.0000 - val_loss: 2.9597 - val_f1_m: 0.6079\n",
      "Epoch 91/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0770e-04 - f1_m: 1.0000 - val_loss: 3.0590 - val_f1_m: 0.6053\n",
      "Epoch 92/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.3645e-05 - f1_m: 1.0000 - val_loss: 3.1168 - val_f1_m: 0.6107\n",
      "Epoch 93/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.4345e-04 - f1_m: 0.9996 - val_loss: 3.1795 - val_f1_m: 0.6139\n",
      "Epoch 94/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.5203e-05 - f1_m: 1.0000 - val_loss: 3.1060 - val_f1_m: 0.6080\n",
      "Epoch 95/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.2605e-05 - f1_m: 1.0000 - val_loss: 3.0914 - val_f1_m: 0.6107\n",
      "Epoch 96/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.1398e-05 - f1_m: 1.0000 - val_loss: 3.1034 - val_f1_m: 0.6062\n",
      "Epoch 97/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2799e-05 - f1_m: 1.0000 - val_loss: 3.1319 - val_f1_m: 0.6040\n",
      "Epoch 98/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.4335e-05 - f1_m: 1.0000 - val_loss: 3.1771 - val_f1_m: 0.5989\n",
      "Epoch 99/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.1175e-05 - f1_m: 1.0000 - val_loss: 3.2274 - val_f1_m: 0.6012\n",
      "Epoch 100/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.5188e-05 - f1_m: 1.0000 - val_loss: 3.2854 - val_f1_m: 0.6012\n",
      "Epoch 101/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7765e-05 - f1_m: 1.0000 - val_loss: 3.3705 - val_f1_m: 0.6100\n",
      "Epoch 102/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.6558e-05 - f1_m: 1.0000 - val_loss: 3.4074 - val_f1_m: 0.6018\n",
      "Epoch 103/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.1443e-05 - f1_m: 0.9924 - val_loss: 3.4518 - val_f1_m: 0.6049\n",
      "Epoch 104/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.7814e-05 - f1_m: 1.0000 - val_loss: 3.5081 - val_f1_m: 0.6058\n",
      "Epoch 105/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.8069e-05 - f1_m: 1.0000 - val_loss: 3.5566 - val_f1_m: 0.6041\n",
      "Epoch 106/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.9410e-05 - f1_m: 1.0000 - val_loss: 3.6391 - val_f1_m: 0.5952\n",
      "Epoch 107/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0273e-05 - f1_m: 1.0000 - val_loss: 3.7131 - val_f1_m: 0.6038\n",
      "Epoch 108/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.6721e-04 - f1_m: 1.0000 - val_loss: 3.7561 - val_f1_m: 0.6003\n",
      "Epoch 109/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.9724e-05 - f1_m: 1.0000 - val_loss: 3.8384 - val_f1_m: 0.6057\n",
      "Epoch 110/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.6643e-06 - f1_m: 1.0000 - val_loss: 3.8823 - val_f1_m: 0.6051\n",
      "Epoch 111/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.4318e-06 - f1_m: 1.0000 - val_loss: 3.8924 - val_f1_m: 0.6095\n",
      "Epoch 112/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4403e-05 - f1_m: 0.9924 - val_loss: 3.9658 - val_f1_m: 0.6003\n",
      "Epoch 113/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.9151e-05 - f1_m: 0.9924 - val_loss: 4.1550 - val_f1_m: 0.6222\n",
      "Epoch 114/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.7372e-06 - f1_m: 1.0000 - val_loss: 4.1704 - val_f1_m: 0.6156\n",
      "Epoch 115/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0991e-05 - f1_m: 1.0000 - val_loss: 4.2322 - val_f1_m: 0.5977\n",
      "Epoch 116/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.5030e-06 - f1_m: 1.0000 - val_loss: 4.2681 - val_f1_m: 0.5977\n",
      "Epoch 117/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.7274e-06 - f1_m: 1.0000 - val_loss: 4.3009 - val_f1_m: 0.6140\n",
      "Epoch 118/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0011 - f1_m: 0.9993 - val_loss: 4.2362 - val_f1_m: 0.5970\n",
      "Epoch 119/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1442 - f1_m: 0.9085 - val_loss: 1.9369 - val_f1_m: 0.0763\n",
      "Epoch 120/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0398 - f1_m: 0.9770 - val_loss: 1.4583 - val_f1_m: 0.5957\n",
      "Epoch 121/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0082 - f1_m: 0.9948 - val_loss: 1.8051 - val_f1_m: 0.5986\n",
      "Epoch 122/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0050 - f1_m: 0.9980 - val_loss: 2.0990 - val_f1_m: 0.5933\n",
      "Epoch 123/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0024 - f1_m: 0.9984 - val_loss: 2.3001 - val_f1_m: 0.5819\n",
      "Epoch 124/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0019 - f1_m: 0.9989 - val_loss: 2.5544 - val_f1_m: 0.5864\n",
      "Epoch 125/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0050 - f1_m: 0.9890 - val_loss: 1.9604 - val_f1_m: 0.5752\n",
      "Epoch 126/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0106 - f1_m: 0.9907 - val_loss: 2.3774 - val_f1_m: 0.6023\n",
      "Epoch 127/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0307 - f1_m: 0.9813 - val_loss: 1.4969 - val_f1_m: 0.5827\n",
      "Epoch 128/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0108 - f1_m: 0.9937 - val_loss: 1.6834 - val_f1_m: 0.5929\n",
      "Epoch 129/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0013 - f1_m: 1.0000 - val_loss: 2.0564 - val_f1_m: 0.6005\n",
      "Epoch 130/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.7291e-04 - f1_m: 1.0000 - val_loss: 2.2640 - val_f1_m: 0.5937\n",
      "Epoch 131/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.0322e-04 - f1_m: 1.0000 - val_loss: 2.4485 - val_f1_m: 0.6079\n",
      "Epoch 132/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.1486e-04 - f1_m: 1.0000 - val_loss: 2.5466 - val_f1_m: 0.6199\n",
      "Epoch 133/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.5137e-04 - f1_m: 1.0000 - val_loss: 2.6401 - val_f1_m: 0.6110\n",
      "Epoch 134/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.7906e-05 - f1_m: 1.0000 - val_loss: 2.6902 - val_f1_m: 0.6088\n",
      "Epoch 135/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8900e-04 - f1_m: 1.0000 - val_loss: 2.7840 - val_f1_m: 0.6098\n",
      "Epoch 136/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.2252e-04 - f1_m: 1.0000 - val_loss: 2.8687 - val_f1_m: 0.6078\n",
      "Epoch 137/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0946e-04 - f1_m: 1.0000 - val_loss: 2.9206 - val_f1_m: 0.6073\n",
      "Epoch 138/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.7594e-05 - f1_m: 1.0000 - val_loss: 2.9745 - val_f1_m: 0.6039\n",
      "Epoch 139/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0087e-04 - f1_m: 1.0000 - val_loss: 3.0305 - val_f1_m: 0.6021\n",
      "Epoch 140/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.5328e-05 - f1_m: 1.0000 - val_loss: 3.1161 - val_f1_m: 0.6000\n",
      "Epoch 141/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.1188e-05 - f1_m: 1.0000 - val_loss: 3.1556 - val_f1_m: 0.5998\n",
      "Epoch 142/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.2897e-05 - f1_m: 1.0000 - val_loss: 3.2136 - val_f1_m: 0.5960\n",
      "Epoch 143/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0843e-04 - f1_m: 1.0000 - val_loss: 3.3130 - val_f1_m: 0.6015\n",
      "Epoch 144/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8623e-04 - f1_m: 0.9996 - val_loss: 3.4252 - val_f1_m: 0.5999\n",
      "Epoch 145/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.3221e-05 - f1_m: 1.0000 - val_loss: 3.4260 - val_f1_m: 0.5982\n",
      "Epoch 146/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.3740e-05 - f1_m: 1.0000 - val_loss: 3.4511 - val_f1_m: 0.6002\n",
      "Epoch 147/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7744e-05 - f1_m: 1.0000 - val_loss: 3.5177 - val_f1_m: 0.5974\n",
      "Epoch 148/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.7124e-05 - f1_m: 1.0000 - val_loss: 3.5978 - val_f1_m: 0.5967\n",
      "Epoch 149/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.3610e-05 - f1_m: 1.0000 - val_loss: 3.6821 - val_f1_m: 0.5990\n",
      "Epoch 150/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2782e-05 - f1_m: 1.0000 - val_loss: 3.7132 - val_f1_m: 0.6004\n",
      "Epoch 151/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.0967e-05 - f1_m: 1.0000 - val_loss: 3.7452 - val_f1_m: 0.6015\n",
      "Epoch 152/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1131e-05 - f1_m: 1.0000 - val_loss: 3.7705 - val_f1_m: 0.6025\n",
      "Epoch 153/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.5956e-05 - f1_m: 1.0000 - val_loss: 3.8487 - val_f1_m: 0.5994\n",
      "Epoch 154/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.0108e-05 - f1_m: 1.0000 - val_loss: 3.9163 - val_f1_m: 0.6023\n",
      "Epoch 155/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7650e-05 - f1_m: 1.0000 - val_loss: 4.0052 - val_f1_m: 0.5987\n",
      "Epoch 156/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.6560e-05 - f1_m: 1.0000 - val_loss: 4.0721 - val_f1_m: 0.6001\n",
      "Epoch 157/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.9558e-06 - f1_m: 1.0000 - val_loss: 4.0897 - val_f1_m: 0.6039\n",
      "Epoch 158/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.3898e-06 - f1_m: 1.0000 - val_loss: 4.1227 - val_f1_m: 0.6017\n",
      "Epoch 159/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.0160e-05 - f1_m: 1.0000 - val_loss: 4.2761 - val_f1_m: 0.5991\n",
      "Epoch 160/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.2377e-06 - f1_m: 1.0000 - val_loss: 4.3450 - val_f1_m: 0.5987\n",
      "Epoch 161/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.5176e-04 - f1_m: 1.0000 - val_loss: 4.3755 - val_f1_m: 0.5987\n",
      "Epoch 162/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.3305e-06 - f1_m: 1.0000 - val_loss: 4.4372 - val_f1_m: 0.6017\n",
      "Epoch 163/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.3897e-06 - f1_m: 0.9924 - val_loss: 4.4412 - val_f1_m: 0.5998\n",
      "Epoch 164/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.5377e-04 - f1_m: 1.0000 - val_loss: 4.4831 - val_f1_m: 0.5973\n",
      "Epoch 165/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.5524e-05 - f1_m: 1.0000 - val_loss: 4.6677 - val_f1_m: 0.6020\n",
      "Epoch 166/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.5375e-06 - f1_m: 1.0000 - val_loss: 4.7533 - val_f1_m: 0.6004\n",
      "Epoch 167/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.4212e-06 - f1_m: 0.9924 - val_loss: 4.8131 - val_f1_m: 0.5977\n",
      "Epoch 168/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.2007e-06 - f1_m: 1.0000 - val_loss: 4.8680 - val_f1_m: 0.5975\n",
      "Epoch 169/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8466e-06 - f1_m: 1.0000 - val_loss: 4.8886 - val_f1_m: 0.5944\n",
      "Epoch 170/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.7281e-06 - f1_m: 1.0000 - val_loss: 4.9720 - val_f1_m: 0.6009\n",
      "Epoch 171/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.1519e-06 - f1_m: 1.0000 - val_loss: 4.9864 - val_f1_m: 0.6028\n",
      "Epoch 172/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.6418e-06 - f1_m: 1.0000 - val_loss: 5.0568 - val_f1_m: 0.5967\n",
      "Epoch 173/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4064e-05 - f1_m: 1.0000 - val_loss: 5.1912 - val_f1_m: 0.5970\n",
      "Epoch 174/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 9.1666e-07 - f1_m: 1.0000 - val_loss: 5.2007 - val_f1_m: 0.5970\n",
      "Epoch 175/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.4880e-06 - f1_m: 1.0000 - val_loss: 5.2203 - val_f1_m: 0.5963\n",
      "Epoch 176/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.3848e-05 - f1_m: 1.0000 - val_loss: 5.4756 - val_f1_m: 0.6106\n",
      "Epoch 177/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.7120e-06 - f1_m: 1.0000 - val_loss: 5.6971 - val_f1_m: 0.6059\n",
      "Epoch 178/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 9.7045e-07 - f1_m: 1.0000 - val_loss: 5.7239 - val_f1_m: 0.6112\n",
      "Epoch 179/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.2681e-07 - f1_m: 1.0000 - val_loss: 5.7529 - val_f1_m: 0.6161\n",
      "Epoch 180/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.8540e-07 - f1_m: 1.0000 - val_loss: 5.7751 - val_f1_m: 0.6177\n",
      "Epoch 181/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.7643e-07 - f1_m: 1.0000 - val_loss: 5.7869 - val_f1_m: 0.6152\n",
      "Epoch 182/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.0904e-06 - f1_m: 1.0000 - val_loss: 5.8194 - val_f1_m: 0.6144\n",
      "Epoch 183/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.8239e-07 - f1_m: 1.0000 - val_loss: 5.8497 - val_f1_m: 0.6159\n",
      "Epoch 184/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.2029e-06 - f1_m: 1.0000 - val_loss: 6.0004 - val_f1_m: 0.6119\n",
      "Epoch 185/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.5779e-06 - f1_m: 1.0000 - val_loss: 5.9939 - val_f1_m: 0.6177\n",
      "Epoch 186/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.6696e-07 - f1_m: 1.0000 - val_loss: 5.9810 - val_f1_m: 0.6164\n",
      "Epoch 187/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4930e-07 - f1_m: 1.0000 - val_loss: 6.0071 - val_f1_m: 0.6208\n",
      "Epoch 188/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4939e-04 - f1_m: 1.0000 - val_loss: 6.0257 - val_f1_m: 0.6059\n",
      "Epoch 189/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.5266e-06 - f1_m: 1.0000 - val_loss: 6.2773 - val_f1_m: 0.6092\n",
      "Epoch 190/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.2282e-07 - f1_m: 1.0000 - val_loss: 6.2609 - val_f1_m: 0.6101\n",
      "Epoch 191/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.9932e-06 - f1_m: 1.0000 - val_loss: 6.4817 - val_f1_m: 0.6103\n",
      "Epoch 192/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1152e-07 - f1_m: 1.0000 - val_loss: 6.5458 - val_f1_m: 0.6062\n",
      "Epoch 193/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.3785e-08 - f1_m: 1.0000 - val_loss: 6.5613 - val_f1_m: 0.6101\n",
      "Epoch 194/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.6201e-06 - f1_m: 1.0000 - val_loss: 6.8897 - val_f1_m: 0.6073\n",
      "Epoch 195/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.0165e-07 - f1_m: 1.0000 - val_loss: 6.9653 - val_f1_m: 0.6082\n",
      "Epoch 196/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8770e-06 - f1_m: 1.0000 - val_loss: 7.0968 - val_f1_m: 0.6082\n",
      "Epoch 197/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.4920e-07 - f1_m: 1.0000 - val_loss: 7.0978 - val_f1_m: 0.6043\n",
      "Epoch 198/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.0111e-07 - f1_m: 1.0000 - val_loss: 7.1061 - val_f1_m: 0.6040\n",
      "Epoch 199/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.6916e-08 - f1_m: 1.0000 - val_loss: 7.0987 - val_f1_m: 0.6079\n",
      "Epoch 200/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.6770e-08 - f1_m: 1.0000 - val_loss: 7.1180 - val_f1_m: 0.6012\n",
      "Epoch 201/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.2184e-07 - f1_m: 1.0000 - val_loss: 7.1326 - val_f1_m: 0.6035\n",
      "Epoch 202/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.8828e-07 - f1_m: 1.0000 - val_loss: 7.2054 - val_f1_m: 0.6103\n",
      "Epoch 203/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4575e-04 - f1_m: 1.0000 - val_loss: 7.2125 - val_f1_m: 0.6108\n",
      "Epoch 204/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0151e-08 - f1_m: 1.0000 - val_loss: 7.2185 - val_f1_m: 0.6108\n",
      "Epoch 205/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.3844e-07 - f1_m: 1.0000 - val_loss: 7.2723 - val_f1_m: 0.6125\n",
      "Epoch 206/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.0071e-06 - f1_m: 1.0000 - val_loss: 7.9465 - val_f1_m: 0.5927\n",
      "Epoch 207/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 9.2979e-08 - f1_m: 1.0000 - val_loss: 7.8245 - val_f1_m: 0.5953\n",
      "Epoch 208/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.8635e-07 - f1_m: 1.0000 - val_loss: 7.8067 - val_f1_m: 0.5932\n",
      "Epoch 209/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.4401e-07 - f1_m: 1.0000 - val_loss: 7.8920 - val_f1_m: 0.5959\n",
      "Epoch 210/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1312e-06 - f1_m: 1.0000 - val_loss: 8.0593 - val_f1_m: 0.5870\n",
      "Epoch 211/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.2121 - f1_m: 0.8684 - val_loss: 1.1685 - val_f1_m: 0.2626\n",
      "Epoch 212/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0769 - f1_m: 0.9490 - val_loss: 4.5085 - val_f1_m: 0.0061\n",
      "Epoch 213/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0157 - f1_m: 0.9891 - val_loss: 2.6838 - val_f1_m: 0.4376\n",
      "Epoch 214/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0064 - f1_m: 0.9967 - val_loss: 3.3909 - val_f1_m: 0.4103\n",
      "Epoch 215/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0029 - f1_m: 0.9908 - val_loss: 2.2386 - val_f1_m: 0.5920\n",
      "Epoch 216/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0012 - f1_m: 0.9924 - val_loss: 2.4744 - val_f1_m: 0.5909\n",
      "Epoch 217/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.3335e-04 - f1_m: 1.0000 - val_loss: 2.6250 - val_f1_m: 0.5796\n",
      "Epoch 218/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.6789e-04 - f1_m: 1.0000 - val_loss: 2.7750 - val_f1_m: 0.5972\n",
      "Epoch 219/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.7445e-04 - f1_m: 1.0000 - val_loss: 2.8699 - val_f1_m: 0.5926\n",
      "Epoch 220/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.6506e-04 - f1_m: 1.0000 - val_loss: 2.9200 - val_f1_m: 0.6019\n",
      "Epoch 221/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.2538e-04 - f1_m: 0.9997 - val_loss: 2.9798 - val_f1_m: 0.5453\n",
      "Epoch 222/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 6.4354e-04 - f1_m: 0.9996 - val_loss: 4.4691 - val_f1_m: 0.5451\n",
      "Epoch 223/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.0289e-04 - f1_m: 1.0000 - val_loss: 3.9017 - val_f1_m: 0.5746\n",
      "Epoch 224/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.6747e-04 - f1_m: 1.0000 - val_loss: 3.5419 - val_f1_m: 0.6069\n",
      "Epoch 225/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8140e-04 - f1_m: 1.0000 - val_loss: 3.9443 - val_f1_m: 0.5947\n",
      "Epoch 226/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.6198e-05 - f1_m: 1.0000 - val_loss: 3.6565 - val_f1_m: 0.5922\n",
      "Epoch 227/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2862e-04 - f1_m: 1.0000 - val_loss: 3.8131 - val_f1_m: 0.5997\n",
      "Epoch 228/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0029 - f1_m: 0.9993 - val_loss: 4.6544 - val_f1_m: 0.5352\n",
      "Epoch 229/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1170 - f1_m: 0.9239 - val_loss: 2.9769 - val_f1_m: 0.4600\n",
      "Epoch 230/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0307 - f1_m: 0.9795 - val_loss: 1.9107 - val_f1_m: 0.3715\n",
      "Epoch 231/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0066 - f1_m: 0.9972 - val_loss: 1.7832 - val_f1_m: 0.6012\n",
      "Epoch 232/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0032 - f1_m: 0.9979 - val_loss: 2.0615 - val_f1_m: 0.5928\n",
      "Epoch 233/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.0127e-04 - f1_m: 0.9997 - val_loss: 2.3084 - val_f1_m: 0.5926\n",
      "Epoch 234/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.0005e-04 - f1_m: 1.0000 - val_loss: 2.4939 - val_f1_m: 0.6035\n",
      "Epoch 235/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.2861e-04 - f1_m: 1.0000 - val_loss: 2.6139 - val_f1_m: 0.6020\n",
      "Epoch 236/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8897e-04 - f1_m: 1.0000 - val_loss: 2.8053 - val_f1_m: 0.5831\n",
      "Epoch 237/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4683e-04 - f1_m: 1.0000 - val_loss: 2.8117 - val_f1_m: 0.5988\n",
      "Epoch 238/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.8327e-05 - f1_m: 1.0000 - val_loss: 2.8638 - val_f1_m: 0.6051\n",
      "Epoch 239/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0278e-04 - f1_m: 1.0000 - val_loss: 2.9823 - val_f1_m: 0.5992\n",
      "Epoch 240/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 6.9873e-04 - f1_m: 0.9997 - val_loss: 3.7974 - val_f1_m: 0.5775\n",
      "Epoch 241/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0285 - f1_m: 0.9809 - val_loss: 2.1276 - val_f1_m: 0.4338\n",
      "Epoch 242/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0379 - f1_m: 0.9793 - val_loss: 2.3731 - val_f1_m: 0.2929\n",
      "Epoch 243/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 0.0083 - f1_m: 0.9960 - val_loss: 2.9816 - val_f1_m: 0.4103\n",
      "Epoch 244/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0025 - f1_m: 0.9982 - val_loss: 3.1594 - val_f1_m: 0.4664\n",
      "Epoch 245/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.5939e-04 - f1_m: 1.0000 - val_loss: 2.6438 - val_f1_m: 0.5739\n",
      "Epoch 246/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.8912e-04 - f1_m: 1.0000 - val_loss: 2.6808 - val_f1_m: 0.5932\n",
      "Epoch 247/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.1453e-04 - f1_m: 1.0000 - val_loss: 2.7030 - val_f1_m: 0.6057\n",
      "Epoch 248/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.3990e-04 - f1_m: 0.9924 - val_loss: 2.8207 - val_f1_m: 0.6077\n",
      "Epoch 249/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8220e-04 - f1_m: 1.0000 - val_loss: 2.9859 - val_f1_m: 0.5972\n",
      "Epoch 250/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0222e-04 - f1_m: 1.0000 - val_loss: 3.0874 - val_f1_m: 0.6008\n",
      "Epoch 251/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.1417e-05 - f1_m: 1.0000 - val_loss: 3.1802 - val_f1_m: 0.6029\n",
      "Epoch 252/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.1530e-05 - f1_m: 1.0000 - val_loss: 3.2187 - val_f1_m: 0.5990\n",
      "Epoch 253/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.9071e-05 - f1_m: 1.0000 - val_loss: 3.2802 - val_f1_m: 0.6028\n",
      "Epoch 254/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.4751e-05 - f1_m: 1.0000 - val_loss: 3.4015 - val_f1_m: 0.5958\n",
      "Epoch 255/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 6.4110e-05 - f1_m: 1.0000 - val_loss: 3.4087 - val_f1_m: 0.6098\n",
      "Epoch 256/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.6904e-05 - f1_m: 1.0000 - val_loss: 3.4823 - val_f1_m: 0.6012\n",
      "Epoch 257/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 8.3317e-05 - f1_m: 1.0000 - val_loss: 3.5920 - val_f1_m: 0.6047\n",
      "Epoch 258/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.7026e-05 - f1_m: 1.0000 - val_loss: 3.6490 - val_f1_m: 0.5978\n",
      "Epoch 259/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2787e-05 - f1_m: 1.0000 - val_loss: 3.6871 - val_f1_m: 0.5981\n",
      "Epoch 260/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.1058e-05 - f1_m: 1.0000 - val_loss: 3.7218 - val_f1_m: 0.5970\n",
      "Epoch 261/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.3035e-05 - f1_m: 1.0000 - val_loss: 3.7795 - val_f1_m: 0.6000\n",
      "Epoch 262/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4714e-04 - f1_m: 1.0000 - val_loss: 3.8114 - val_f1_m: 0.6011\n",
      "Epoch 263/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.2375e-06 - f1_m: 1.0000 - val_loss: 3.8646 - val_f1_m: 0.6000\n",
      "Epoch 264/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.3222e-05 - f1_m: 1.0000 - val_loss: 3.8908 - val_f1_m: 0.5978\n",
      "Epoch 265/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 5.0192e-05 - f1_m: 0.9924 - val_loss: 3.9748 - val_f1_m: 0.6029\n",
      "Epoch 266/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.2538e-05 - f1_m: 1.0000 - val_loss: 4.0334 - val_f1_m: 0.6049\n",
      "Epoch 267/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.7540e-05 - f1_m: 1.0000 - val_loss: 4.0018 - val_f1_m: 0.6133\n",
      "Epoch 268/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.7322e-05 - f1_m: 1.0000 - val_loss: 4.1361 - val_f1_m: 0.5932\n",
      "Epoch 269/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 7.9146e-06 - f1_m: 1.0000 - val_loss: 4.1645 - val_f1_m: 0.6011\n",
      "Epoch 270/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.8374e-05 - f1_m: 1.0000 - val_loss: 4.3932 - val_f1_m: 0.5875\n",
      "Epoch 271/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 6.2588e-06 - f1_m: 1.0000 - val_loss: 4.9067 - val_f1_m: 0.5749\n",
      "Epoch 272/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.3290e-05 - f1_m: 0.9924 - val_loss: 4.5882 - val_f1_m: 0.5908\n",
      "Epoch 273/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.6842e-06 - f1_m: 1.0000 - val_loss: 4.5464 - val_f1_m: 0.5952\n",
      "Epoch 274/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.0752e-05 - f1_m: 1.0000 - val_loss: 4.5732 - val_f1_m: 0.6032\n",
      "Epoch 275/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.6991e-06 - f1_m: 1.0000 - val_loss: 4.6162 - val_f1_m: 0.5992\n",
      "Epoch 276/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.9668e-06 - f1_m: 1.0000 - val_loss: 4.7066 - val_f1_m: 0.6004\n",
      "Epoch 277/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 5.1512e-06 - f1_m: 1.0000 - val_loss: 4.7112 - val_f1_m: 0.6040\n",
      "Epoch 278/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.8488e-06 - f1_m: 1.0000 - val_loss: 4.7117 - val_f1_m: 0.6016\n",
      "Epoch 279/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.4491e-06 - f1_m: 1.0000 - val_loss: 4.7738 - val_f1_m: 0.5964\n",
      "Epoch 280/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.6587e-06 - f1_m: 1.0000 - val_loss: 4.8091 - val_f1_m: 0.6079\n",
      "Epoch 281/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.9690e-06 - f1_m: 1.0000 - val_loss: 4.8985 - val_f1_m: 0.6031\n",
      "Epoch 282/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 7.5287e-06 - f1_m: 1.0000 - val_loss: 4.9478 - val_f1_m: 0.5982\n",
      "Epoch 283/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.6071e-06 - f1_m: 1.0000 - val_loss: 5.0076 - val_f1_m: 0.5940\n",
      "Epoch 284/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.9429e-06 - f1_m: 1.0000 - val_loss: 5.1168 - val_f1_m: 0.5923\n",
      "Epoch 285/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 5.2312e-06 - f1_m: 1.0000 - val_loss: 5.2398 - val_f1_m: 0.5888\n",
      "Epoch 286/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.3043e-06 - f1_m: 0.9924 - val_loss: 5.2679 - val_f1_m: 0.5865\n",
      "Epoch 287/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.4235e-04 - f1_m: 0.9924 - val_loss: 5.2843 - val_f1_m: 0.5944\n",
      "Epoch 288/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1072e-05 - f1_m: 1.0000 - val_loss: 6.0873 - val_f1_m: 0.5853\n",
      "Epoch 289/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0715 - f1_m: 0.9643 - val_loss: 2.4397 - val_f1_m: 0.0000e+00\n",
      "Epoch 290/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1108 - f1_m: 0.9269 - val_loss: 1.0168 - val_f1_m: 0.5416\n",
      "Epoch 291/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0208 - f1_m: 0.9873 - val_loss: 2.6428 - val_f1_m: 0.4168\n",
      "Epoch 292/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0045 - f1_m: 0.9980 - val_loss: 2.0669 - val_f1_m: 0.5546\n",
      "Epoch 293/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0012 - f1_m: 1.0000 - val_loss: 2.2338 - val_f1_m: 0.5942\n",
      "Epoch 294/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.7379e-04 - f1_m: 1.0000 - val_loss: 2.4910 - val_f1_m: 0.5893\n",
      "Epoch 295/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.2319e-04 - f1_m: 1.0000 - val_loss: 2.5600 - val_f1_m: 0.5868\n",
      "Epoch 296/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8237e-04 - f1_m: 1.0000 - val_loss: 2.6509 - val_f1_m: 0.5820\n",
      "Epoch 297/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.0677e-05 - f1_m: 1.0000 - val_loss: 2.6986 - val_f1_m: 0.5826\n",
      "Epoch 298/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.9698e-04 - f1_m: 1.0000 - val_loss: 2.8091 - val_f1_m: 0.5955\n",
      "Epoch 299/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.3340e-04 - f1_m: 1.0000 - val_loss: 2.9230 - val_f1_m: 0.5895\n",
      "Epoch 300/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.5769e-05 - f1_m: 1.0000 - val_loss: 2.9582 - val_f1_m: 0.5886\n",
      "Epoch 301/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.6947e-04 - f1_m: 1.0000 - val_loss: 3.0639 - val_f1_m: 0.5915\n",
      "Epoch 302/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.9926e-05 - f1_m: 0.9924 - val_loss: 3.1642 - val_f1_m: 0.5949\n",
      "Epoch 303/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1904e-04 - f1_m: 1.0000 - val_loss: 3.2319 - val_f1_m: 0.5837\n",
      "Epoch 304/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.4366e-05 - f1_m: 1.0000 - val_loss: 3.3374 - val_f1_m: 0.5848\n",
      "Epoch 305/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.0054e-05 - f1_m: 1.0000 - val_loss: 3.4196 - val_f1_m: 0.5865\n",
      "Epoch 306/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.4742e-05 - f1_m: 1.0000 - val_loss: 3.4935 - val_f1_m: 0.5964\n",
      "Epoch 307/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.5452e-05 - f1_m: 1.0000 - val_loss: 3.4592 - val_f1_m: 0.5824\n",
      "Epoch 308/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7457e-05 - f1_m: 1.0000 - val_loss: 3.5170 - val_f1_m: 0.5873\n",
      "Epoch 309/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.3205e-05 - f1_m: 1.0000 - val_loss: 3.5806 - val_f1_m: 0.5890\n",
      "Epoch 310/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.7692e-06 - f1_m: 1.0000 - val_loss: 3.6107 - val_f1_m: 0.5881\n",
      "Epoch 311/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 3.3518e-05 - f1_m: 1.0000 - val_loss: 3.6735 - val_f1_m: 0.5878\n",
      "Epoch 312/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.0650e-05 - f1_m: 1.0000 - val_loss: 3.7026 - val_f1_m: 0.5917\n",
      "Epoch 313/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0147e-05 - f1_m: 1.0000 - val_loss: 3.7834 - val_f1_m: 0.5862\n",
      "Epoch 314/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.2909e-05 - f1_m: 0.9924 - val_loss: 3.9345 - val_f1_m: 0.5833\n",
      "Epoch 315/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.6372e-05 - f1_m: 1.0000 - val_loss: 3.9714 - val_f1_m: 0.5886\n",
      "Epoch 316/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.4555e-05 - f1_m: 1.0000 - val_loss: 4.0020 - val_f1_m: 0.5924\n",
      "Epoch 317/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.7558e-06 - f1_m: 1.0000 - val_loss: 4.0068 - val_f1_m: 0.5856\n",
      "Epoch 318/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.4222e-05 - f1_m: 0.9924 - val_loss: 4.2406 - val_f1_m: 0.5765\n",
      "Epoch 319/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 9.6799e-06 - f1_m: 1.0000 - val_loss: 4.1787 - val_f1_m: 0.5805\n",
      "Epoch 320/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.9806e-05 - f1_m: 1.0000 - val_loss: 4.2179 - val_f1_m: 0.5868\n",
      "Epoch 321/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2673e-05 - f1_m: 1.0000 - val_loss: 4.3038 - val_f1_m: 0.6053\n",
      "Epoch 322/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.6213e-05 - f1_m: 1.0000 - val_loss: 4.3502 - val_f1_m: 0.5953\n",
      "Epoch 323/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.4671e-05 - f1_m: 1.0000 - val_loss: 4.4078 - val_f1_m: 0.5904\n",
      "Epoch 324/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.1275e-04 - f1_m: 0.9992 - val_loss: 4.4803 - val_f1_m: 0.5880\n",
      "Epoch 325/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.9407e-05 - f1_m: 1.0000 - val_loss: 4.6409 - val_f1_m: 0.5913\n",
      "Epoch 326/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.5664e-05 - f1_m: 1.0000 - val_loss: 4.6988 - val_f1_m: 0.5810\n",
      "Epoch 327/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.1878e-06 - f1_m: 1.0000 - val_loss: 4.7190 - val_f1_m: 0.5826\n",
      "Epoch 328/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.4457e-05 - f1_m: 1.0000 - val_loss: 4.8451 - val_f1_m: 0.5776\n",
      "Epoch 329/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.1659e-06 - f1_m: 1.0000 - val_loss: 4.8963 - val_f1_m: 0.5824\n",
      "Epoch 330/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.6411e-06 - f1_m: 1.0000 - val_loss: 4.9021 - val_f1_m: 0.5899\n",
      "Epoch 331/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.3974e-04 - f1_m: 1.0000 - val_loss: 4.9495 - val_f1_m: 0.5876\n",
      "Epoch 332/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0493e-05 - f1_m: 1.0000 - val_loss: 4.9198 - val_f1_m: 0.5815\n",
      "Epoch 333/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.2029e-06 - f1_m: 1.0000 - val_loss: 5.1110 - val_f1_m: 0.5939\n",
      "Epoch 334/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 5.6032e-06 - f1_m: 1.0000 - val_loss: 5.2610 - val_f1_m: 0.5887\n",
      "Epoch 335/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8073e-05 - f1_m: 1.0000 - val_loss: 5.3179 - val_f1_m: 0.5820\n",
      "Epoch 336/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.7037e-06 - f1_m: 1.0000 - val_loss: 5.3044 - val_f1_m: 0.5903\n",
      "Epoch 337/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.7647e-07 - f1_m: 1.0000 - val_loss: 5.2906 - val_f1_m: 0.5805\n",
      "Epoch 338/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.5283e-06 - f1_m: 0.9924 - val_loss: 5.3095 - val_f1_m: 0.5812\n",
      "Epoch 339/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8016e-05 - f1_m: 1.0000 - val_loss: 5.4692 - val_f1_m: 0.5877\n",
      "Epoch 340/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.3682e-04 - f1_m: 1.0000 - val_loss: 5.5507 - val_f1_m: 0.5853\n",
      "Epoch 341/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 7.0020e-06 - f1_m: 1.0000 - val_loss: 5.6600 - val_f1_m: 0.5810\n",
      "Epoch 342/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 1.0789e-06 - f1_m: 1.0000 - val_loss: 5.7051 - val_f1_m: 0.5819\n",
      "Epoch 343/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.3210e-06 - f1_m: 1.0000 - val_loss: 5.6089 - val_f1_m: 0.5855\n",
      "Epoch 344/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.2108e-07 - f1_m: 1.0000 - val_loss: 5.7239 - val_f1_m: 0.5817\n",
      "Epoch 345/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.0998e-07 - f1_m: 1.0000 - val_loss: 5.7732 - val_f1_m: 0.5795\n",
      "Epoch 346/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 4.3161e-07 - f1_m: 1.0000 - val_loss: 5.7648 - val_f1_m: 0.5807\n",
      "Epoch 347/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.6324e-06 - f1_m: 1.0000 - val_loss: 5.9089 - val_f1_m: 0.5796\n",
      "Epoch 348/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0862 - f1_m: 0.9597 - val_loss: 3.9400 - val_f1_m: 0.0000e+00\n",
      "Epoch 349/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.1090 - f1_m: 0.9270 - val_loss: 1.9347 - val_f1_m: 0.1715\n",
      "Epoch 350/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0223 - f1_m: 0.9897 - val_loss: 1.7151 - val_f1_m: 0.4855\n",
      "Epoch 351/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0035 - f1_m: 0.9995 - val_loss: 1.7210 - val_f1_m: 0.5789\n",
      "Epoch 352/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0018 - f1_m: 0.9994 - val_loss: 3.9615 - val_f1_m: 0.2809\n",
      "Epoch 353/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0047 - f1_m: 0.9969 - val_loss: 2.0755 - val_f1_m: 0.5948\n",
      "Epoch 354/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0086 - f1_m: 0.9930 - val_loss: 4.5955 - val_f1_m: 0.3482\n",
      "Epoch 355/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0060 - f1_m: 0.9878 - val_loss: 2.4048 - val_f1_m: 0.5935\n",
      "Epoch 356/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0036 - f1_m: 0.9986 - val_loss: 2.3504 - val_f1_m: 0.5893\n",
      "Epoch 357/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0028 - f1_m: 0.9982 - val_loss: 2.5083 - val_f1_m: 0.5724\n",
      "Epoch 358/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0063 - f1_m: 0.9961 - val_loss: 3.4349 - val_f1_m: 0.4029\n",
      "Epoch 359/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0076 - f1_m: 0.9920 - val_loss: 1.9226 - val_f1_m: 0.5454\n",
      "Epoch 360/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0198 - f1_m: 0.9877 - val_loss: 5.2490 - val_f1_m: 0.2090\n",
      "Epoch 361/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0046 - f1_m: 0.9899 - val_loss: 2.2155 - val_f1_m: 0.5709\n",
      "Epoch 362/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0018 - f1_m: 0.9971 - val_loss: 2.4233 - val_f1_m: 0.6046\n",
      "Epoch 363/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 6.9067e-04 - f1_m: 0.9924 - val_loss: 2.5723 - val_f1_m: 0.6086\n",
      "Epoch 364/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.2959e-04 - f1_m: 1.0000 - val_loss: 2.8287 - val_f1_m: 0.5928\n",
      "Epoch 365/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.0288e-04 - f1_m: 1.0000 - val_loss: 2.9626 - val_f1_m: 0.6085\n",
      "Epoch 366/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.8725e-04 - f1_m: 1.0000 - val_loss: 3.0399 - val_f1_m: 0.5989\n",
      "Epoch 367/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.4835e-05 - f1_m: 1.0000 - val_loss: 3.0985 - val_f1_m: 0.5958\n",
      "Epoch 368/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 2.1773e-04 - f1_m: 1.0000 - val_loss: 3.2447 - val_f1_m: 0.5980\n",
      "Epoch 369/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.2060e-05 - f1_m: 1.0000 - val_loss: 3.2966 - val_f1_m: 0.5974\n",
      "Epoch 370/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 9.4579e-05 - f1_m: 1.0000 - val_loss: 3.3712 - val_f1_m: 0.6019\n",
      "Epoch 371/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2574e-04 - f1_m: 1.0000 - val_loss: 3.9529 - val_f1_m: 0.5218\n",
      "Epoch 372/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 2.8700e-04 - f1_m: 1.0000 - val_loss: 3.4835 - val_f1_m: 0.6027\n",
      "Epoch 373/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0061 - f1_m: 0.9972 - val_loss: 13.5112 - val_f1_m: 0.1156\n",
      "Epoch 374/1000\n",
      "132/132 [==============================] - 1s 7ms/step - loss: 0.0349 - f1_m: 0.9767 - val_loss: 1.1178 - val_f1_m: 0.5597\n",
      "Epoch 375/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 0.0090 - f1_m: 0.9949 - val_loss: 2.5848 - val_f1_m: 0.5207\n",
      "Epoch 376/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 8.5664e-04 - f1_m: 1.0000 - val_loss: 2.2299 - val_f1_m: 0.5739\n",
      "Epoch 377/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.8851e-04 - f1_m: 1.0000 - val_loss: 2.9443 - val_f1_m: 0.5468\n",
      "Epoch 378/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 7.3212e-04 - f1_m: 1.0000 - val_loss: 2.6110 - val_f1_m: 0.5766\n",
      "Epoch 379/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 4.8390e-04 - f1_m: 0.9997 - val_loss: 3.1700 - val_f1_m: 0.5773\n",
      "Epoch 380/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 2.9171e-04 - f1_m: 1.0000 - val_loss: 3.2323 - val_f1_m: 0.5848\n",
      "Epoch 381/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 3.0198e-04 - f1_m: 1.0000 - val_loss: 3.5940 - val_f1_m: 0.5609\n",
      "Epoch 382/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2044e-04 - f1_m: 1.0000 - val_loss: 3.6534 - val_f1_m: 0.5819\n",
      "Epoch 383/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 5.0943e-05 - f1_m: 1.0000 - val_loss: 3.6373 - val_f1_m: 0.5807\n",
      "Epoch 384/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 1.2890e-04 - f1_m: 1.0000 - val_loss: 3.8060 - val_f1_m: 0.5747\n",
      "Epoch 385/1000\n",
      "132/132 [==============================] - 1s 9ms/step - loss: 1.4317e-04 - f1_m: 1.0000 - val_loss: 3.8696 - val_f1_m: 0.5870\n",
      "Epoch 386/1000\n",
      "132/132 [==============================] - 1s 8ms/step - loss: 3.0098e-05 - f1_m: 1.0000 - val_loss: 3.9128 - val_f1_m: 0.5804\n",
      "Epoch 387/1000\n",
      " 68/132 [==============>...............] - ETA: 0s - loss: 2.4989e-05 - f1_m: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[f1_m])\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "def create_model():\n",
    "    x = Input(shape=(20, 20, 3))\n",
    "    x1 = Conv2D(16, (3, 3), padding='same', activation='relu')(x)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = MaxPooling2D(pool_size=(2, 2), padding='same')(x1)\n",
    "    x2 = Conv2D(32, (5, 5), padding='same', activation='relu')(x1)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = MaxPooling2D(pool_size=(2, 2), padding='same')(x2)\n",
    "    x3 = Conv2D(32, (5, 5), padding='same', activation='relu')(x2)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = MaxPooling2D(pool_size=(2, 2), padding='same')(x3)\n",
    "    x4 = Conv2D(64, (5, 5), padding='same', activation='relu')(x3)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = MaxPooling2D(pool_size=(4, 4), padding='same')(x4)\n",
    "    y = Flatten()(x4)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(32)(y)\n",
    "    y = LeakyReLU(alpha=0.1)(y)\n",
    "    y = Dense(16)(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(1, activation='sigmoid')(y)\n",
    "    return tf.keras.Model(inputs=x, outputs=y)\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_m])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=1000, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55776fb2-7ecf-4e0d-bd5f-572376b2e654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = np.where(y_pred_proba > 0.5, 1, 0)\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "print(\"F1 score on the validation set:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d489bba4-3549-4fba-8d1b-1ce7ae75efec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1b8bc-e1d7-4760-9711-b31a74c48ac2",
   "metadata": {},
   "source": [
    "## Supervised series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e4a2c0a-5048-4aac-9766-cffdaaa357cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: F1 Score = 0.5690915409772377\n",
      "Random Forest: F1 Score = 0.45226782589262376\n",
      "XGBoost: F1 Score = 0.5698222598094936\n",
      "KNN: F1 Score = 0.6642723106907817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: F1 Score = 0.6219035677561024\n",
      "SVM: F1 Score = 0.4683916624918131\n",
      "Naive Bayes: F1 Score = 0.6527170284542594\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the models\n",
    "models = [\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"XGBoost\", XGBClassifier()),\n",
    "    (\"KNN\", KNeighborsClassifier()),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Naive Bayes\", GaussianNB())\n",
    "]\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='f1')\n",
    "    print(f\"{model_name}: F1 Score = {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "42ca8147-71da-45d3-834a-d85bfb2b718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for KNN: {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
      "Best F1 Score for KNN: 0.7557257130860456\n",
      "Best Hyperparameters for Naive Bayes: {'var_smoothing': 0.1}\n",
      "Best F1 Score for Naive Bayes: 0.6763954037011807\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 13, 21, 50, 100],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn_grid = GridSearchCV(knn, knn_params, cv=10, scoring='f1')\n",
    "knn_grid.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and F1 score for KNN\n",
    "print(\"Best Hyperparameters for KNN:\", knn_grid.best_params_)\n",
    "print(\"Best F1 Score for KNN:\", knn_grid.best_score_)\n",
    "\n",
    "# Define the parameter grid for Naive Bayes\n",
    "nb_params = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-5, 1e-3, 1e-1]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb_grid = GridSearchCV(nb, nb_params, cv=10, scoring='f1')\n",
    "nb_grid.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and F1 score for Naive Bayes\n",
    "print(\"Best Hyperparameters for Naive Bayes:\", nb_grid.best_params_)\n",
    "print(\"Best F1 Score for Naive Bayes:\", nb_grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dd799bd-370f-4300-bf6f-ab9241874e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Naive Bayes: {'var_smoothing': 0.1}\n",
      "Best F1 Score for Naive Bayes: 0.6763954037011807\n",
      "Best Hyperparameters for Bernoulli Naive Bayes: {'alpha': 0.5, 'binarize': 1.0, 'fit_prior': True}\n",
      "Best F1 Score for Bernoulli Naive Bayes: 0.6442755278325175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define the parameter grid for Naive Bayes\n",
    "nb_params = {\n",
    "    'var_smoothing': [1e-9, 1e-5, 1e-3, 1e-1, 0]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for Naive Bayes\n",
    "nb = GaussianNB()\n",
    "nb_grid = GridSearchCV(nb, nb_params, cv=10, scoring='f1')\n",
    "nb_grid.fit(X, y)\n",
    "\n",
    "# Print the best hyperparameters and F1 score for Naive Bayes\n",
    "print(\"Best Hyperparameters for Naive Bayes:\", nb_grid.best_params_)\n",
    "print(\"Best F1 Score for Naive Bayes:\", nb_grid.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb_params = {\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'fit_prior': [True, False],\n",
    "    'binarize': [0.0, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb_grid = GridSearchCV(bnb, bnb_params, cv=10, scoring='f1')\n",
    "bnb_grid.fit(X, y)\n",
    "\n",
    "print(\"Best Hyperparameters for Bernoulli Naive Bayes:\", bnb_grid.best_params_)\n",
    "print(\"Best F1 Score for Bernoulli Naive Bayes:\", bnb_grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
