## README: Fake Photo Detection Model

This README provides clear instructions on how to reproduce the predictions generated by the Fake Photo Detection Model. It includes information about data pre-processing, feature extraction, model training, and prediction generation. The README also provides details about the development environment, data files used, the algorithm employed, its main hyperparameters, and any other relevant comments.

## Environment Details

The model was developed and trained in the following environment:

* Operating System: macOS
* Jupyter Notebook: Installed and used as the development platform
* Python Version: 3.11
* Memory (RAM): 16 GB
* CPU: Utilized for model training and predictions

## Data Files

The model utilizes two provided datasets provided by Bitgrit:

* train.csv: This file contains the training data used to train the machine learning model. It includes the features and labels necessary for training. The target variable 'labels' has values 0 for 'real' photos and 1 for 'fake' photos.
* test.csv: This file is used to evaluate the model's performance on unseen data. It contains only the features, and the goal is to generate predictions for the corresponding labels.


## Models

### K-Nearest Neighbors (KNN) Model

* Code file: Submissions_KNN.ipynb
* Summary: This model utilizes the K-Nearest Neighbors algorithm for classification. It is trained on the provided train.csv dataset and used to make predictions on the test.csv dataset. The model is configured with 7 neighbors, Manhattan distance metric (p=1), and uniform weights.

### Gaussian Naive Bayes Model

* Code file: Submissions_NBayes.ipynb
* Summary: This model employs the Gaussian Naive Bayes algorithm for classification. It is trained on the train.csv dataset and used to predict labels for the test.csv dataset. The model is configured with a var_smoothing parameter of 0.1.

### Modified MesoNet Model

* Code file: Submissions_ModifiedMeso.ipynb
* Summary: This model is a modified version of the MesoNet architecture. It is trained on the train.csv dataset, which is pre-processed and reshaped to fit the model's required input shape. The model is trained using Adam optimizer, binary cross-entropy loss, and F1 score as a metric. After training, it is used to generate predictions for the test.csv dataset.
